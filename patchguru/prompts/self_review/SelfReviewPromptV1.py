from patchguru.utils.Tracker import Event, append_event

class SelfReviewPrompt:
    def __init__(self):
        pass

    def create_prompt(
        self,
        pull_request_details,
        prev_fut_code,
        post_fut_signatures,
        enclosing_class,
        test_driver,
        error_message,
        code_changes,
    ):
        template = """
# Role
You are an expert software developer. Your task is to analyze a test driver that is generated by an AI system for testing a pull request (PR) in a codebase. The test driver is failing with assertion errors when run against the new code introduced by the PR. Your goal is to determine whether the failure indicates a real bug in the PR or a flawed test driver that does not accurately reflect the developer's intent.

# Guidelines

Your task is to act as a detective, distinguishing between a real bug and a flawed test. Here are guidelines to help you analyze failing test drivers against a new PR.

1.  **Read the PR Details Carefully:** Start with the developer's own words. The PR description and any linked issues/PRs are your first source of truth. Look for clues about what the developer intended the code to do. What problem does the PR aim to solve? Are there any known limitations or edge cases mentioned?

2. **Read Code Changes Carefully:** Examine the code changes introduced by the PR. Understand what the new code is supposed to do, how it interacts with existing code, and any new assumptions it introduces. Pay attention to test cases added or modified in the PR, as they often reflect the developer's intent.

3.  **Examine the Test's Intent:** Read the failing test itself. What is it trying to prove? Pay close attention to the test's name, comments, and the specific assertion that is failing. Is the test checking for a specific value, a particular data type, or a behavioral change?

4.  **Cross-Reference the Test Driver with PR Details:** Compare the failing test with the expected behavior as described in the PR. Does the test align with the developer's stated intent? If the test expects behavior that contradicts the PR description, that's a strong indicator of a mismatch.

5.  **Analyze the Failure Data:** Look at the actual versus expected values in the assertion error. The difference between what the test expected and what the code produced is the most concrete piece of evidence you have. Sometimes the difference is a small detailâ€”a changed data type, a rounding error, or a slightly different output format. This is often where a mismatch in intent becomes clear.

6.  **Identify Documentation Gaps:** If the PR's behavior deviates from what the tests assume, but you believe the new behavior is correct and intentional, the problem might be insufficient documentation. A good PR should clearly explain any breaking changes or new assumptions. If it doesn't, that's a potential bug in the PR's submission, not the code itself.

7.  **Consider Unintended Side Effects:** The bug might not be in the feature the PR is directly addressing. The changes could have a knock-on effect on another part of the codebase that the failing test is checking. A test failure in an unrelated area is a strong indicator of a real, unintended bug.

8.  **Look for Alternative Interpretations:** If the PR description is vague, consider alternative ways the code could be interpreted. Could the developer have a different understanding of a term or a function's purpose? This requires empathy and a willingness to see the problem from their perspective.

9. **Formulate a Clear Conclusion:** Based on your analysis, state your findings clearly. Is it a **bug** (the code is not doing what it should) or a **mismatch** (the test is out of date or based on incorrect assumptions)? Provide evidence from your analysis and recommend a clear path forward, such as fixing the code, updating the test, or clarifying the PR's documentation.

10. **Suggest Corrections if Needed:** If you identify a mismatch, suggest how the test driver could be updated to better reflect the intended behavior of the code. You might recommend changes to the test's assertions, input values, or even its overall purpose. You MUST NOT suggest changes to the PR code itself. If you find a bug, you should not do anything further than identifying it, as fixing the bug is outside the scope of your role.

# Input

You will be provided the test driver, the error message from the failed test, pull request details, including the PR title, description, and conversations between developers, and code changes introduced by the PR, as follows:

## Pull Request Details

{pr_details}

## Code Changes Introduced by the Pull Request

```
{code_changes}
```

## Test Driver

```python
{test_driver}
```

## Error Message

{error_message}

# Output Format Instructions

Provide your response in the following format:

<reasoning>
[Put your reasoning chain here including the analysis of the pull request details, the function code before the PR, and the test driver. This should include your thought process on how you arrived at your conclusion.]
</reasoning>

<conclusion>
[State your conclusion here: "BUG" if the failure indicates a real bug in the PR, or "MISMATCH" if the failure is due to a flawed test. Only put "BUG" or "MISMATCH" here in uppercase letters.]
</conclusion>

<test_driver>
[Put the corrected test driver here if you concluded "MISMATCH". Make sure that fixed code is structurally similar to the input code. The fixed code should still contain three main sections: "# Neccessary imports", "# Specification", and "# Source Code of target function(s)". You can assume that the "# Source Code of target function(s)" section is unchanged and do not need to provide it again in your response. But you should still include the section header "# Source Code of target function(s)" between # Neccessary imports and # Specification sections in your response. If you concluded "BUG", you MUST leave this section empty.]
</test_driver>
        """
        if len(enclosing_class) > 0:
            if len(enclosing_class) > 3000:
                enclosing_class = enclosing_class[:3000]
                enclosing_class += "(...truncated...)"
            context = f"""
Target function (s) are defined in the following class:

```python
{enclosing_class}
```
"""
        else:
            context = "Target function(s) are defined in the global scope. There is no enclosing class."
        query = template.format(
            pr_details=pull_request_details,
            prev_fut_code=prev_fut_code,
            context=context,
            post_fut_signatures=post_fut_signatures,
            test_driver=test_driver,
            error_message=error_message,
            code_changes=code_changes
        )
        return query

    def parse_answer(self, answer):
        results = {}
        if "<reasoning>" not in answer or "</reasoning>" not in answer:
            append_event(Event(
                level="WARNING",
                message="Missing required tag: <reasoning>"
            ))
        else:
            reasoning_start = answer.index("<reasoning>") + len("<reasoning>")
            reasoning_end = answer.index("</reasoning>")
            results["reasoning"] = answer[reasoning_start:reasoning_end].strip()

        if "<conclusion>" not in answer or "</conclusion>" not in answer:
            append_event(Event(
                level="ERROR",
                message="Missing required tag: <conclusion>"
            ))
            return None

        if "<test_driver>" not in answer or "</test_driver>" not in answer:
            append_event(Event(
                level="ERROR",
                message="LLM response is missing required tag: <test_driver>"
            ))
            return None

        try:
            conclusion_start = answer.index("<conclusion>") + len("<conclusion>")
            conclusion_end = answer.index("</conclusion>")
            results["conclusion"] = answer[conclusion_start:conclusion_end].strip().upper()

            specification_start = answer.index("<test_driver>") + len("<test_driver>")
            specification_end = answer.index("</test_driver>")
            specification = answer[specification_start:specification_end].strip()

            if "```" in specification:
                if "```python" in specification:
                    specification = specification.split("```python")[1].split("```")[0].strip()
                else:
                    specification = specification.split("```")[1].split("```")[0].strip()
            results["specification"] = specification

        except ValueError as e:
            append_event(Event(
                level="ERROR",
                message=f"Error while parsing LLM response: {e}"
            ))
            return None

        return results

    def check_valid(self, parsed_response, func_name):
        conclusion = parsed_response["conclusion"]
        if conclusion not in ["BUG", "MISMATCH"]:
            append_event(Event(
                level="DEBUG",
                message="Conclusion is not valid. It should be either 'BUG' or 'MISMATCH'."
            ))
            return False
        if conclusion == "BUG":
            return True

        specification = parsed_response["specification"]
        if "# Source Code of target function(s)" not in specification or "# Specification" not in specification:
            append_event(Event(
                level="DEBUG",
                message="Specification is missing required sections."
            ))
            return False
        test_driver = specification.split("# Specification")[1].strip()
        pre_function_name = "pre_" + func_name
        post_function_name = "post_" + func_name
        if pre_function_name not in test_driver or post_function_name not in test_driver:
            append_event(Event(
                level="DEBUG",
                message="Test driver did not call to the target function(s) directly."
            ))
            return False

        append_event(Event(
            level="DEBUG",
            message="Specification passed basic validity checks."
        ))
        return True

    def hidden_post_pr_code(self, code):
        assert "## After Pull Request" in code, "The provided code does not contain the expected section header '# After Pull Request'."
        assert "# Specification" in code, "The provided code does not contain the expected section header '# Specification'."
        before = code.split("## After Pull Request")[0]
        after = code.split("# Specification")[1]
        new_code = f"""
{before}
## After Pull Request
### [...hidden to avoid bias...]

# Specification
{after}
"""
        return new_code

    def insert_code(self, prev_fut_code, post_fut_code, specification):
        """
        Inserts concrete function code into the specification.
        """
        if "# Source Code of target function(s)" not in specification or "# Specification" not in specification:
            append_event(Event(
                level="ERROR",
                message="Specification format is incorrect. Missing required sections."
            ))
            return None

        import_part = specification.split("# Source Code of target function(s)")[0].strip()
        spec_part = specification.split("# Specification")[1].strip()

        completed_specification = f"""
{import_part}

# Source Code of target function(s)

## Before Pull Request
{prev_fut_code}

## After Pull Request
{post_fut_code}

# Specification
{spec_part}
        """
        return completed_specification
